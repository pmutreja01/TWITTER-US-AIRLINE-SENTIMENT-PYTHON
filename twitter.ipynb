{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Airline Sentiment Analysis using Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Project Description</b>\n",
    "\n",
    "<em> Twitter data was scraped from February of 2015 and contributors were asked to classify positive, negative, and neutral tweets.Next we are training our model on this data to classify future tweets into relevant categories.As part of this project we are tokenizing tweets into meaningful words and based on these words we are training our model.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SHAPE : vectorized 14640 tweets. found 11626 terms.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "#nltk.download()\n",
    "data = pd.read_csv('.\\Data\\Tweets.csv')\n",
    "# Data Preprocessing\n",
    "data['text']=data['text'].map(lambda x: re.sub(\"^@[^\\s]+\\s\",\"\",x))\n",
    "def getHashtag(x):\n",
    "    g=re.match(\"^[^#]+#([^\\s]+).*\",x)\n",
    "    if g:\n",
    "        return g.group(1)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "data['hashtags']=data['text'].map(getHashtag)\n",
    "\n",
    "data['hashtags']=data['hashtags'].str.lower() \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "def data_preprocess( raw_review ):\n",
    "    # Function for data preprocessing like removing urls, removing special charachters, stop words etc.\n",
    "    review_text = re.sub('((www\\S+)|(http\\S+))',\" \", raw_review)\n",
    "           \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    \n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    return( \" \".join(meaningful_words))   \n",
    "\n",
    "Reviews=[]\n",
    "for i in range(0, len(data)):\n",
    "    Reviews.append(data_preprocess(data['text'].tolist()[i]))\n",
    "#print(Reviews)\n",
    "vectorizer = CountVectorizer(min_df=1,stop_words='english')\n",
    "Reviews\n",
    "X = vectorizer.fit_transform(Reviews)\n",
    "#print(X)\n",
    "#print(X.toarray())\n",
    "#print(X.shape)\n",
    "#print(X[1,0:1])\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "print('DATA SHAPE : vectorized %d tweets. found %d terms.' % (X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2 : Project - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORMANCE MEASURE : ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected Accuracy as our performance measure for various classifiers as we have multiclass problem for which best performance measure we can use is accuracy only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline : DummyClassifier() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for predicting majority class 0.627\n",
      "Accuracy for random prediction 0.460\n"
     ]
    }
   ],
   "source": [
    "base_clf = DummyClassifier(strategy='most_frequent',random_state=0)\n",
    "Acc_Score = cross_val_score(base_clf,X,data['airline_sentiment'], scoring='f1_micro',cv=10)\n",
    "print(\"Accuracy for predicting majority class %0.3f\" % np.mean(Acc_Score))\n",
    "base_clf = DummyClassifier(strategy='stratified',random_state=0)\n",
    "Acc_Score = cross_val_score(base_clf,X,data['airline_sentiment'], scoring='accuracy',cv=10)\n",
    "print(\"Accuracy for random prediction %0.3f\" % np.mean(Acc_Score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 : LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION ACCURACIES FOR VARIOUS EXPERMENTS:\n",
      "0.762\n",
      "0.757\n",
      "0.762\n",
      "0.737\n",
      "0.761\n",
      "0.739\n",
      "0.762\n",
      "0.689\n",
      "0.754\n"
     ]
    }
   ],
   "source": [
    "Y = data['airline_sentiment']\n",
    "lr_clf = LogisticRegression(random_state = 2)\n",
    "lr_clf.fit(X,Y)\n",
    "#predicted = cross_val_predict(clf,X,Y, cv=10)\n",
    "#print(predicted)\n",
    "print(\"LOGISTIC REGRESSION ACCURACIES FOR VARIOUS EXPERIMENTS:\")\n",
    "Acc_Score = cross_val_score(lr_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "lr_clf = LogisticRegression(penalty = 'l2',C=1,solver = 'lbfgs', multi_class = 'multinomial')\n",
    "lr_clf.fit(X,Y)\n",
    "Acc_Score = cross_val_score(lr_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "lr_clf = LogisticRegression(penalty = 'l2',C=0.5,solver = 'lbfgs', multi_class = 'multinomial',warm_start=True)\n",
    "lr_clf.fit(X,Y)\n",
    "Acc_Score = cross_val_score(lr_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "lr_clf = LogisticRegression(penalty = 'l2',C=10,solver = 'lbfgs', multi_class = 'multinomial')\n",
    "lr_clf.fit(X,Y)\n",
    "Acc_Score = cross_val_score(lr_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "lr_clf = LogisticRegression(penalty = 'l2',C=0.5,solver = 'newton-cg', multi_class = 'multinomial')\n",
    "lr_clf.fit(X,Y)\n",
    "Acc_Score = cross_val_score(lr_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "lr_clf = LogisticRegression(penalty = 'l2',C=8,solver = 'newton-cg', multi_class = 'multinomial')\n",
    "lr_clf.fit(X,Y)\n",
    "Acc_Score = cross_val_score(lr_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "lr_clf = LogisticRegression(penalty = 'l2',C=0.5,solver = 'sag', multi_class = 'multinomial')\n",
    "lr_clf.fit(X,Y)\n",
    "Acc_Score = cross_val_score(lr_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "lr_clf = LogisticRegression(penalty = 'l2',C=0.01,solver = 'sag', multi_class = 'multinomial')\n",
    "lr_clf.fit(X,Y)\n",
    "Acc_Score = cross_val_score(lr_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "lr_clf = LogisticRegression(penalty = 'l2',C=1.5,solver = 'sag', multi_class = 'multinomial', max_iter = 200)\n",
    "lr_clf.fit(X,Y)\n",
    "Acc_Score = cross_val_score(lr_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 : MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTINOMIAL NB ACCURACIES FOR VARIOUS EXPERIMENTS:\n",
      "0.741\n",
      "0.740\n",
      "0.721\n",
      "0.729\n",
      "0.661\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X, Y)\n",
    "print(\"MULTINOMIAL NB ACCURACIES FOR VARIOUS EXPERIMENTS:\")\n",
    "Acc_Score = cross_val_score(nb_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "#print(clf.class_count_)\n",
    "#print(clf.feature_count_)\n",
    "nb_clf = MultinomialNB(alpha=0.5)\n",
    "nb_clf.fit(X, Y)\n",
    "Acc_Score = cross_val_score(nb_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "nb_clf = MultinomialNB(alpha = 0.5,fit_prior=False)\n",
    "nb_clf.fit(X, Y)\n",
    "Acc_Score = cross_val_score(nb_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "nb_clf = MultinomialNB(alpha=0.1)\n",
    "nb_clf.fit(X, Y)\n",
    "Acc_Score = cross_val_score(nb_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))\n",
    "nb_clf = MultinomialNB(alpha=10)\n",
    "nb_clf.fit(X, Y)\n",
    "Acc_Score = cross_val_score(nb_clf,X,Y, scoring='accuracy',cv=10)\n",
    "print(\"%0.3f\" % np.mean(Acc_Score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 : DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE ACCURACIES FOR VARIOUS EXPERIMENTS:\n",
      "0.680\n",
      "0.678\n",
      "0.675\n",
      "0.688\n",
      "0.689\n",
      "0.683\n"
     ]
    }
   ],
   "source": [
    "#Setting No Parameters in Decision Tree Classifier\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf = dt_clf.fit(X, Y)\n",
    "print(\"DECISION TREE ACCURACIES FOR VARIOUS EXPERIMENTS:\")\n",
    "dt_acc_score = cross_val_score(dt_clf, X, Y , scoring='accuracy', cv=10)\n",
    "print(\"%0.3f\" % np.mean(dt_acc_score))\n",
    "#random_state = 0\n",
    "dt_clf = DecisionTreeClassifier(criterion = \"entropy\")\n",
    "dt_clf = dt_clf.fit(X, Y)\n",
    "dt_acc_score = cross_val_score(dt_clf, X, Y , scoring='accuracy', cv=10)\n",
    "print(\"%0.3f\" % np.mean(dt_acc_score))\n",
    "dt_clf = DecisionTreeClassifier(criterion = \"entropy\", max_features = 2500)\n",
    "dt_clf = dt_clf.fit(X, Y)\n",
    "dt_acc_score = cross_val_score(dt_clf, X, Y , scoring='accuracy', cv=10)\n",
    "print(\"%0.3f\" % np.mean(dt_acc_score))\n",
    "dt_clf = DecisionTreeClassifier(criterion = \"entropy\", splitter = \"random\", max_depth = 15)\n",
    "dt_clf = dt_clf.fit(X, Y)\n",
    "dt_acc_score = cross_val_score(dt_clf, X, Y , scoring='accuracy', cv=10)\n",
    "print(\"%0.3f\" % np.mean(dt_acc_score))\n",
    "dt_clf = DecisionTreeClassifier(criterion = \"gini\", splitter = \"random\", max_depth = 20)\n",
    "dt_clf = dt_clf.fit(X, Y)\n",
    "dt_acc_score = cross_val_score(dt_clf, X, Y , scoring='accuracy', cv=10)\n",
    "print(\"%0.3f\" % np.mean(dt_acc_score))\n",
    "dt_clf = DecisionTreeClassifier(criterion = \"entropy\",  max_depth = 10)\n",
    "dt_clf = dt_clf.fit(X, Y)\n",
    "dt_acc_score = cross_val_score(dt_clf, X, Y , scoring='accuracy', cv=10)\n",
    "print(\"%0.3f\" % np.mean(dt_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top weighted terms for positive class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('excellent', 2.4698606530671827),\n",
       " ('thank', 2.4524533861460607),\n",
       " ('awesome', 2.4107220780222267),\n",
       " ('kudos', 2.1142145589555761),\n",
       " ('thanks', 2.0872604728012187),\n",
       " ('exceptional', 2.0129560900800607),\n",
       " ('great', 1.929478465066081),\n",
       " ('thnx', 1.9191855954324339),\n",
       " ('wonderful', 1.8975481323911223),\n",
       " ('amazing', 1.8974496237912515)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the top weighted features?\n",
    "\n",
    "# Get the learned coefficients for the Positive class.\n",
    "coef = lr_clf.coef_[2]\n",
    "# Sort them in descending order.\n",
    "top_coef_ind = np.argsort(coef)[::-1][:10]\n",
    "# Get the names of those features.\n",
    "top_coef_terms = vocab[top_coef_ind]\n",
    "# Get the weights of those features\n",
    "top_coef = coef[top_coef_ind]\n",
    "# Print the top 10.\n",
    "print('top weighted terms for positive class:')\n",
    "[x for x in zip(top_coef_terms, top_coef)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top weighted terms for neutral class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hungupnohelp', 1.9155857503257268),\n",
       " ('apple', 1.4739055254424387),\n",
       " ('vincenzolandino', 1.4319373059688365),\n",
       " ('fb', 1.41724032198453),\n",
       " ('impression', 1.3788949421081247),\n",
       " ('cal', 1.3671670220853933),\n",
       " ('dominican', 1.3445820820167644),\n",
       " ('resolutions', 1.306348379225978),\n",
       " ('mexico', 1.2961076680447825),\n",
       " ('oak', 1.2840069977125725)]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the top weighted features?\n",
    "\n",
    "# Get the learned coefficients for the Neutral class.\n",
    "coef = lr_clf.coef_[1]\n",
    "# Sort them in descending order.\n",
    "top_coef_ind = np.argsort(coef)[::-1][:10]\n",
    "# Get the names of those features.\n",
    "top_coef_terms = vocab[top_coef_ind]\n",
    "# Get the weights of those features\n",
    "top_coef = coef[top_coef_ind]\n",
    "# Print the top 10.\n",
    "print('top weighted terms for neutral class:')\n",
    "[x for x in zip(top_coef_terms, top_coef)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top weighted terms for negative class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('worst', 2.7373202402557539),\n",
       " ('ridiculous', 1.9272283663170076),\n",
       " ('solution', 1.8801381503463053),\n",
       " ('worse', 1.8553713120843336),\n",
       " ('fail', 1.8076866440564201),\n",
       " ('unacceptable', 1.7483931546413329),\n",
       " ('disappointed', 1.7325686851391822),\n",
       " ('terrible', 1.6647084455668986),\n",
       " ('rude', 1.6543094536538918),\n",
       " ('ruining', 1.6297928250415386)]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the top weighted features?\n",
    "\n",
    "# Get the learned coefficients for the Negative class.\n",
    "coef = lr_clf.coef_[0]\n",
    "# Sort them in descending order.\n",
    "top_coef_ind = np.argsort(coef)[::-1][:10]\n",
    "# Get the names of those features.\n",
    "top_coef_terms = vocab[top_coef_ind]\n",
    "# Get the weights of those features\n",
    "top_coef = coef[top_coef_ind]\n",
    "# Print the top 10.\n",
    "print('top weighted terms for negative class:')\n",
    "[x for x in zip(top_coef_terms, top_coef)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_clf = MLPClassifier(random_state=2)\n",
    "mlp_clf = mlp_clf.fit(X,Y)\n",
    "mlp_acc_score = cross_val_score(mlp_clf, X, Y, cv=5, scoring='accuracy')\n",
    "print(\"%0.3f\" % np.mean(mlp_acc_score))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
